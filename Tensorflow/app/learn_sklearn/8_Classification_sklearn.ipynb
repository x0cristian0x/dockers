{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import tree\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report, RocCurveDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from xgboost import plot_tree\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, cross_val_predict, GridSearchCV\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import rcParams\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"font.size\"] = 8\n",
    "plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "plt.rcParams[\"figure.facecolor\"] = \"#00000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "wine = datasets.load_wine()\n",
    "x, y = pd.DataFrame(wine.data, columns=wine.feature_names), pd.DataFrame(wine.target, columns=[\"class\"])\n",
    "y_labels = wine.target_names\n",
    "pd.concat([x,y], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([x,y], axis=1).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k folders\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# metrics url = https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "scoring = [\"accuracy\", \"f1_macro\",\"recall_macro\"]\n",
    "\n",
    "# split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for model\n",
    "def function_model(model, x, y, scoring, **kwargs):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    train_scores = pd.DataFrame(cross_validate(model, x_train, y_train, cv=kfold, scoring=scoring))\n",
    "    test_scores = pd.DataFrame(cross_validate(model, x_test, y_test, cv=kfold, scoring=scoring))\n",
    "    return pd.concat([train_scores.mean(), test_scores.mean()], keys=[\"Train Mean\",\"Test Mean\"],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic model with Grid\n",
    "param_grid = {'C': [0.1, 1, 10, 100],  'penalty': [ 'l1','l2']}\n",
    "\n",
    "logistic_model= LogisticRegression(max_iter=1000, solver=\"liblinear\").fit(x_train, y_train.squeeze()) # tiene warning, usar squeeze\n",
    "\n",
    "train_scores = pd.DataFrame(cross_validate(GridSearchCV(estimator=logistic_model, param_grid=param_grid), \n",
    "                                           x_train, y_train.squeeze(), cv=kfold, scoring=scoring))\n",
    "test_scores = pd.DataFrame(cross_validate(GridSearchCV(estimator=logistic_model, param_grid=param_grid), \n",
    "                                          x_test, y_test.squeeze(), cv=kfold, scoring=scoring))\n",
    "pd.concat([train_scores.mean(), test_scores.mean()], keys=[\"Train Mean\",\"Test Mean\"],  axis=1).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic model\n",
    "logistic_model= LogisticRegression(max_iter=1000, solver=\"newton-cg\").fit(x_train, y_train.squeeze()) # tiene warning, usar squeeze\n",
    "train_scores = pd.DataFrame(cross_validate(logistic_model, x_train, y_train.squeeze(), cv=kfold, scoring=scoring))\n",
    "test_scores = pd.DataFrame(cross_validate(logistic_model, x_test, y_test.squeeze(), cv=kfold, scoring=scoring))\n",
    "pd.concat([train_scores.mean(), test_scores.mean()], keys=[\"Train Mean\",\"Test Mean\"],  axis=1).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example  of ROC\n",
    "wine = datasets.load_wine()\n",
    "target_names = wine.target_names\n",
    "x, y = wine.data, wine.target\n",
    "y = wine.target_names[y]\n",
    "\n",
    "# k folders\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, stratify=y, random_state=0)\n",
    "\n",
    "# Create model and kfold\n",
    "logistic_model= LogisticRegression(max_iter=1000, solver=\"newton-cg\").fit(x_train, y_train)\n",
    "train_scores = cross_val_predict(logistic_model, x_train, y_train, cv=kfold, method='predict_proba')\n",
    "\n",
    "# variable transform of number to label\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)  # (n_samples, n_classes)\n",
    "\n",
    "class_of_interest = \"class_2\"\n",
    "label_binarizer.transform([ class_of_interest])\n",
    "class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "\n",
    "# Plot ROC\n",
    "display = RocCurveDisplay.from_predictions(y_onehot_test[:, class_id],\n",
    "                                           train_scores[:, class_id],\n",
    "                                           name=f\"{class_of_interest} vs the rest\",\n",
    "                                           color=\"darkorange\",\n",
    "                                           plot_chance_level=True )\n",
    "\n",
    "_ = display.ax_.set(xlabel=\"False Positive Rate\",\n",
    "                    ylabel=\"True Positive Rate\",\n",
    "                    title=\"One-vs-Rest ROC curves:\\class_2 vs (class_0 & class_1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits model\n",
    "function_model(model=LogisticRegression(max_iter=1000, solver=\"newton-cg\"), x=x, y=y, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine model\n",
    "function_model(SVC(C=2, max_iter=1000), x=x,y=y, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes  model\n",
    "function_model(GaussianNB(), x=x, y=y, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes  model\n",
    "gnb_model = GaussianNB().fit(x_train, y_train)\n",
    "train_scores = pd.DataFrame(cross_validate(gnb_model, x_train, y_train, cv=kfold, scoring=scoring) )\n",
    "test_scores = pd.DataFrame(cross_validate(gnb_model, x_test, y_test, cv=kfold, scoring=scoring) )\n",
    "pd.concat([train_scores.mean(), test_scores.mean()], keys=[\"Train Mean\",\"Test Mean\"],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuadraticDiscriminantAnalysis model\n",
    "qda_model = QuadraticDiscriminantAnalysis().fit(x_train, y_train)\n",
    "train_scores = pd.DataFrame(cross_validate(qda_model, x_train, y_train, cv=kfold, scoring=scoring) )\n",
    "test_scores = pd.DataFrame(cross_validate(qda_model, x_test, y_test, cv=kfold, scoring=scoring) )\n",
    "pd.concat([train_scores.mean(), test_scores.mean()], keys=[\"Train Mean\",\"Test Mean\"],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decisions tree\n",
    "tree_model = tree.DecisionTreeClassifier(max_depth=3, random_state=42).fit(x_train, y_train)\n",
    "train_scores = pd.DataFrame(cross_validate(tree_model, x_train, y_train, cv=kfold, scoring=scoring) )\n",
    "test_scores = pd.DataFrame(cross_validate(tree_model, x_test, y_test, cv=kfold, scoring=scoring) )\n",
    "#pd.concat([train_scores.mean(), test_scores.mean()], keys=[\"Train Mean\",\"Test Mean\"],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc_model = RandomForestClassifier(n_estimators=100).fit(x_train, y_train)\n",
    "train_scores = pd.DataFrame(cross_validate(rfc_model, x_train, y_train, cv=kfold, scoring=scoring) )\n",
    "test_scores = pd.DataFrame(cross_validate(rfc_model, x_test, y_test, cv=kfold, scoring=scoring) )\n",
    "pd.concat([train_scores.mean(), test_scores.mean()], keys=[\"Train Mean\",\"Test Mean\"],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gbc_model = GradientBoostingClassifier(n_estimators=100).fit(x_train, y_train)\n",
    "test_pred = gbc_model.predict(x_test)\n",
    "print(classification_report(y_test, test_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging Classifier\n",
    "bg_model = BaggingClassifier(n_estimators=100).fit(x_train, y_train)\n",
    "test_pred = bg_model.predict(x_test)\n",
    "print(classification_report(y_test, test_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Suport vector model\n",
    "lsvc_model = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(x_train, y_train)\n",
    "test_pred = lsvc_model.predict(x_test)\n",
    "print(classification_report(y_test, test_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos el conjunto de datos\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Agregamos características categóricas falsas\n",
    "X = np.hstack((X, np.random.choice(['A', 'B', 'C'], size=(X.shape[0], 1))))\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Definimos las transformaciones para las características numéricas\n",
    "numeric_features = [0, 1, 2, 3]  # Índices de las características numéricas\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Definimos las transformaciones para las características categóricas\n",
    "categorical_features = [4]  # Índice de la característica categórica\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    #('imputer', CategoricalImputer()),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combinamos las transformaciones utilizando ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Creamos un pipeline que incluye el preprocesamiento y el modelo\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Entrenamos el pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluamos el rendimiento del modelo\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miss Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer,KNNImputer, IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create values nan\n",
    "np.random.seed(42)\n",
    "values_random = np.random.choice(len(X),5)\n",
    "X[values_random] = np.nan\n",
    "print(\"Values miss id: \", values_random)\n",
    "print(\"Number of Values nan: \", np.isnan(X).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[[102, 92, 14, 106, 71]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(fill_value=np.nan, strategy=\"mean\")\n",
    "imputer.fit(X)\n",
    "X_sin_nan = imputer.transform(X)\n",
    "print(f\"X 2 number values nan: \", np.isnan(X_sin_nan).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sin_nan[[102, 92, 14, 106, 71]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miss Values with KNN y IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=3, weights=\"uniform\")\n",
    "imputer.fit_transform(X)\n",
    "X_knn= imputer.transform(X)\n",
    "X_knn[[102, 92, 14, 106, 71]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(random_state=42, max_iter=10) # mejor q knn\n",
    "imputer.fit_transform(X)\n",
    "X_iterative= imputer.transform(X)\n",
    "X_iterative[[102, 92, 14, 106, 71]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(iris.data, X_iterative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
